---
title: "RからローカルLLMをつかう"
author: "伊東宏樹"
date: 2025-11-15
lang: ja
format:
  revealjs:
    theme: [default, custom.scss]
    code-copy: false
knitr:
  opts_chunk: 
    collapse: true
    comment: ""
    prompt: true
execute: 
  echo: true
fig-width: 4
fig-height: 2.6
code-line-numbers: false
embed-resources: true
slide-number: true
editor: visual
---

## この発表の内容

-   ローカルLLMをOllamaを経由して、Rから使う
    -   Rでは、ellmerパッケージを使用

## ローカルLLM

-   自分のコンピューターで動く大規模言語モデル (LLM: Large Language Model)
    -   ネットにつながらなくても使える
    -   データが外部に送信されない
        -   機密情報も扱える
    -   無料で使えるものも多い

## Ollama

-   ローカルLLMを管理・実行するためのツール
-   <https://ollama.com/>
-   Ollamaから使えるローカルLLM
    -   llama
    -   gemma
    -   gpt-oss
    -   その他多数

## Ollamaのインストール

-   インストーラーをダウンロードして、インストール

-   Dockerイメージもあり

## ローカルLLMのダウンロード

::: {style="margin-top: 1em;"}
例: コマンドラインからgemma3nを取得
:::

```{bash}
#| eval: false
#| prompt: false

$ ollama pull gemma3n
```

::: {style="margin-top: 1em;"}
Rから[ollamar](https://cran.r-project.org/package=ollamar)パッケージを使う場合
:::

```{r}
#| eval: false

ollamar::pull("gemma3n")
```

## ellmerパッケージ

-   LLMをRから簡単に使えるようにする
-   <https://ellmer.tidyverse.org/>
-   CRANからインストール可能

```{r}
#| label: install_ellmer
#| eval: false

install.packages("ellmer")
```

## ellmerからローカルLLMを使う

::: {style="margin-top: 1em;"}
チャットの設定
:::

```{r}
#| label: ollama-gemma

library(ellmer)
chat <- chat_ollama(model = "gemma3n")
```

## 実行例

```{r}
#| label: ollama-gemma-example
#| cache: true

chat$chat("Rで分散分析をおこなう関数を教えて")
```

## まとめ

::: {style="margin-top: 0.5em;"}
RからローカルLLMを使用する方法
:::

-   Ollama

    -   別にインストール

    -   使用するローカルLLMを取得しておく

-   ellmerパッケージ

    -   `chat_ollama`関数を使用

        -   `model`引数に、使用するモデルを指定
